{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC-n1mZrlamq",
        "outputId": "68ad412f-1a76-4e3a-cf5d-041ffd05a7a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in c:\\users\\shivansh kalra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shivansh kalra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shivansh kalra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shivansh kalra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shivansh kalra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shivansh kalra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shivansh kalra\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx5EOIw1liL0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qvU9uqaRlPk2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Shivansh Kalra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Model, GPT2Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7sTQBx7lPk3",
        "outputId": "20fa6686-373b-4f39-b930-290a1bf401c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: C:\\Users\\Shivansh Kalra\\.cache\\kagglehub\\datasets\\ethancratchley\\email-phishing-dataset\\versions\\1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ethancratchley/email-phishing-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4n1Od3lflPk3"
      },
      "outputs": [],
      "source": [
        "data_file_path=\"C:\\\\Development\\\\LLMProj\\\\NewProj\\\\email_phishing_data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "UPKkngKTlPk3",
        "outputId": "ee2276c7-c9a6-4721-db21-4d3cf24c2bf9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_words</th>\n",
              "      <th>num_unique_words</th>\n",
              "      <th>num_stopwords</th>\n",
              "      <th>num_links</th>\n",
              "      <th>num_unique_domains</th>\n",
              "      <th>num_email_addresses</th>\n",
              "      <th>num_spelling_errors</th>\n",
              "      <th>num_urgent_keywords</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>140</td>\n",
              "      <td>94</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34</td>\n",
              "      <td>32</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524841</th>\n",
              "      <td>782</td>\n",
              "      <td>327</td>\n",
              "      <td>301</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524842</th>\n",
              "      <td>36</td>\n",
              "      <td>30</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524843</th>\n",
              "      <td>61</td>\n",
              "      <td>46</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524844</th>\n",
              "      <td>213</td>\n",
              "      <td>136</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524845</th>\n",
              "      <td>26</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>524846 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        num_words  num_unique_words  num_stopwords  num_links  \\\n",
              "0             140                94             52          0   \n",
              "1               5                 5              1          0   \n",
              "2              34                32             15          0   \n",
              "3               6                 6              2          0   \n",
              "4               9                 9              2          0   \n",
              "...           ...               ...            ...        ...   \n",
              "524841        782               327            301          2   \n",
              "524842         36                30             11          0   \n",
              "524843         61                46             11          0   \n",
              "524844        213               136             89          0   \n",
              "524845         26                24              3          0   \n",
              "\n",
              "        num_unique_domains  num_email_addresses  num_spelling_errors  \\\n",
              "0                        0                    0                    0   \n",
              "1                        0                    0                    0   \n",
              "2                        0                    0                    0   \n",
              "3                        0                    0                    0   \n",
              "4                        0                    0                    0   \n",
              "...                    ...                  ...                  ...   \n",
              "524841                   2                    2                   52   \n",
              "524842                   0                    0                    4   \n",
              "524843                   0                    0                    3   \n",
              "524844                   0                    0                   18   \n",
              "524845                   0                    0                    2   \n",
              "\n",
              "        num_urgent_keywords  label  \n",
              "0                         0      0  \n",
              "1                         0      0  \n",
              "2                         0      0  \n",
              "3                         0      0  \n",
              "4                         0      0  \n",
              "...                     ...    ...  \n",
              "524841                    1      0  \n",
              "524842                    0      1  \n",
              "524843                    0      0  \n",
              "524844                    0      0  \n",
              "524845                    0      0  \n",
              "\n",
              "[524846 rows x 9 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(data_file_path, sep=\",\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-D9Pl8flPk3",
        "outputId": "cded71d5-260b-41b8-c7d0-47ac94014cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "0    517897\n",
            "1      6949\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df[\"label\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_XQHEKilPk3",
        "outputId": "3d8c8169-eac8-424f-bb92-f78ed7e927d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['num_words', 'num_unique_words', 'num_stopwords', 'num_links',\n",
              "       'num_unique_domains', 'num_email_addresses', 'num_spelling_errors',\n",
              "       'num_urgent_keywords', 'label'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUxxEw5DlPk4",
        "outputId": "ac2cc6ee-bc6a-421f-dd03-b94bfd589072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "0    6949\n",
            "1    6949\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def create_balanced_dataset(df):\n",
        "\n",
        "    num_spam = df[df[\"label\"] == 1].shape[0]\n",
        "    ham_subset = df[df[\"label\"] == 0].sample(num_spam, random_state=123)\n",
        "\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"label\"] == 1]])\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"label\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1W5SVsfclPk4"
      },
      "outputs": [],
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "    # Shuffle the entire DataFrame\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    # Split the DataFrame\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnnhDb8ZlPk4",
        "outputId": "27a080da-8c81-4090-e2cb-8a557031fdd0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['num_words', 'num_unique_words', 'num_stopwords', 'num_links',\n",
              "       'num_unique_domains', 'num_email_addresses', 'num_spelling_errors',\n",
              "       'num_urgent_keywords', 'label'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K7hp1T3klPk4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(193)\n",
        "np.random.seed(193)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0leaAU6hlPk4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class PhishingDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=128):\n",
        "        \"\"\"\n",
        "        Initialize the PhishingDataset.\n",
        "\n",
        "        Args:\n",
        "            csv_file (str): Path to the CSV file containing features and labels.\n",
        "            tokenizer: Tokenizer used to encode the constructed text.\n",
        "            max_length (int): Maximum token length for the model input.\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Feature columns in the dataset\n",
        "        self.feature_cols = [\n",
        "            'num_words', 'num_unique_words', 'num_stopwords', 'num_links',\n",
        "            'num_unique_domains', 'num_email_addresses', 'num_spelling_errors',\n",
        "            'num_urgent_keywords'\n",
        "        ]\n",
        "\n",
        "        # Check if all expected columns exist and fill missing values with 0\n",
        "        for col in self.feature_cols + ['label']:\n",
        "            if col not in self.data.columns:\n",
        "                self.data[col] = 0  # or raise KeyError(f\"Column '{col}' not found in dataset\")\n",
        "            # Fill NaN values with 0\n",
        "            self.data[col] = self.data[col].fillna(0)\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} samples from {csv_file}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _create_text_representation(self, row):\n",
        "        \"\"\"\n",
        "        Create a textual representation of the features for the transformer model.\n",
        "\n",
        "        Args:\n",
        "            row: A pandas Series containing the features\n",
        "\n",
        "        Returns:\n",
        "            str: A textual representation of the features\n",
        "        \"\"\"\n",
        "        text = (\n",
        "            f\"Email characteristics: \"\n",
        "            f\"Contains {row['num_words']} words with {row['num_unique_words']} unique words. \"\n",
        "            f\"Has {row['num_stopwords']} stopwords. \"\n",
        "            f\"Includes {row['num_links']} links to {row['num_unique_domains']} different domains. \"\n",
        "            f\"Contains {row['num_email_addresses']} email addresses. \"\n",
        "            f\"Has {row['num_spelling_errors']} spelling errors. \"\n",
        "            f\"Contains {row['num_urgent_keywords']} urgent keywords.\"\n",
        "        )\n",
        "        return text\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get tokenized text and label for a sample at index idx.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (input_ids, attention_mask, label_tensor)\n",
        "        \"\"\"\n",
        "        # Get the sample\n",
        "        sample = self.data.iloc[idx]\n",
        "\n",
        "        # Create text representation from features\n",
        "        text = self._create_text_representation(sample)\n",
        "\n",
        "        # Ensure we always have a valid text string\n",
        "        if not isinstance(text, str) or not text:\n",
        "            text = \"Email characteristics: Empty or invalid email.\"\n",
        "            print(f\"Warning: Invalid text at index {idx}. Using default text.\")\n",
        "\n",
        "        # Encode the text for the transformer - with explicit error handling\n",
        "        try:\n",
        "            encoding = self.tokenizer.encode(text)\n",
        "            print(f\"DEBUG: Encoding type: {type(encoding)}, value: {encoding}\")\n",
        "\n",
        "            # Check if encoding is None or not a list/sequence\n",
        "            if encoding is None:\n",
        "                print(f\"Warning: Tokenizer returned None encoding for index {idx}, text: {text}\")\n",
        "                encoding = [0]  # Default fallback\n",
        "            elif not isinstance(encoding, (list, tuple, np.ndarray)):\n",
        "                print(f\"Warning: Tokenizer returned non-sequence encoding: {type(encoding)} for index {idx}\")\n",
        "                # Try to convert to list if it's something else\n",
        "                try:\n",
        "                    encoding = list(encoding)\n",
        "                except:\n",
        "                    encoding = [0]  # Default fallback\n",
        "        except Exception as e:\n",
        "            print(f\"Error encoding text at index {idx}: {e}\")\n",
        "            print(f\"Text was: {text}\")\n",
        "            encoding = [0]  # Default fallback\n",
        "\n",
        "        # Make absolutely sure encoding is a list before checking length\n",
        "        if not isinstance(encoding, list):\n",
        "            try:\n",
        "                encoding = list(encoding)\n",
        "            except:\n",
        "                encoding = [0]  # Last resort fallback\n",
        "\n",
        "        # Ensure encoding is not empty\n",
        "        if not encoding:\n",
        "            encoding = [0]\n",
        "\n",
        "        # Truncate or pad to max_length\n",
        "        if len(encoding) > self.max_length:\n",
        "            encoding = encoding[:self.max_length]\n",
        "        else:\n",
        "            # Pad with EOS token ID or 0 depending on tokenizer\n",
        "            pad_token = 0  # Default\n",
        "            if hasattr(self.tokenizer, 'pad_token_id') and self.tokenizer.pad_token_id is not None:\n",
        "                pad_token = self.tokenizer.pad_token_id\n",
        "            encoding = encoding + [pad_token] * (self.max_length - len(encoding))\n",
        "\n",
        "        # Convert to tensors\n",
        "        input_ids = torch.tensor(encoding, dtype=torch.long)\n",
        "        attention_mask = torch.ones_like(input_ids)  # All tokens are real (not padding)\n",
        "\n",
        "        # Extract label\n",
        "        label = torch.tensor(sample['label'], dtype=torch.long)\n",
        "\n",
        "        return input_ids, attention_mask, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fUIwIYB2lPk4"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy_loader(data_loader, model, device):\n",
        "    \"\"\"\n",
        "    Calculate accuracy over a dataloader.\n",
        "\n",
        "    Args:\n",
        "        data_loader: DataLoader to iterate over\n",
        "        model: Model to evaluate\n",
        "        device: Device to run on\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy (0-1)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total if total > 0 else 0\n",
        "\n",
        "def train_classifier(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq=1, eval_iter=None):\n",
        "    \"\"\"\n",
        "    Train a classifier model.\n",
        "\n",
        "    Args:\n",
        "        model: The model to train\n",
        "        train_loader: DataLoader for training data\n",
        "        val_loader: DataLoader for validation data\n",
        "        optimizer: Optimizer for training\n",
        "        device: Device to train on ('cuda' or 'cpu')\n",
        "        num_epochs: Number of epochs to train\n",
        "        eval_freq: How often to evaluate on validation set (in epochs)\n",
        "        eval_iter: Number of iterations to evaluate on (None = all)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (trained model, training losses, validation accuracies)\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        # Unpack all three items from your dataset\n",
        "        for input_ids, attention_mask, labels in train_loader:\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
        "\n",
        "            # Move inputs to the device\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # Evaluation phase\n",
        "        if (epoch + 1) % eval_freq == 0:\n",
        "            accuracy = evaluate_classifier(model, val_loader, device, max_iter=eval_iter)\n",
        "            val_accuracies.append(accuracy)\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model, train_losses, val_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aA7w2qtDlPk5"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_data, labels, model, device):\n",
        "    \"\"\"\n",
        "    Calculate loss for a batch of data.\n",
        "\n",
        "    Args:\n",
        "        input_data: Tuple of (input_ids, attention_mask) or just input tensor\n",
        "        labels: Target labels\n",
        "        model: The model to use\n",
        "        device: Device to use for computation\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Loss value\n",
        "    \"\"\"\n",
        "    # Move data to the correct device\n",
        "    if isinstance(input_data, tuple):\n",
        "        # Unpack the tuple if it contains multiple inputs\n",
        "        input_ids, attention_mask = input_data\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass with both inputs\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    else:\n",
        "        # Single input tensor\n",
        "        input_data = input_data.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass with single input\n",
        "        outputs = model(input_data)\n",
        "\n",
        "    # Extract logits from outputs\n",
        "    logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gy-2jggNlPk5"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nNZzyb5YlPk5"
      },
      "outputs": [],
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    \"\"\"\n",
        "    Calculate average loss over a dataloader.\n",
        "\n",
        "    Args:\n",
        "        data_loader: DataLoader to iterate over\n",
        "        model: Model to evaluate\n",
        "        device: Device to run on\n",
        "        num_batches: Number of batches to use (None = all)\n",
        "\n",
        "    Returns:\n",
        "        float: Average loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            loss = calc_loss_batch(batch[0], batch[2], model, device)  # Corrected call\n",
        "            total_loss += loss\n",
        "            count += 1\n",
        "\n",
        "            if num_batches is not None and i >= num_batches - 1:\n",
        "                break\n",
        "\n",
        "    return total_loss / count if count > 0 else float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oJr6cisalPk5"
      },
      "outputs": [],
      "source": [
        "def train_classifier(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter):\n",
        "    # Initialize lists to track losses and examples seen\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_ids, attention_mask, labels in train_loader:\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch((input_ids, attention_mask), labels, model, device)\n",
        "            loss.backward()  # Calculate loss gradients\n",
        "            optimizer.step()  # Update model weights using loss gradients\n",
        "\n",
        "            examples_seen += input_ids.shape[0]  # Track examples\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Calculate accuracy after each epoch\n",
        "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "41SIBwYylPk6"
      },
      "outputs": [],
      "source": [
        "#ADDED\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeatureBasedClassifier(nn.Module):\n",
        "    def __init__(self, input_size=8, hidden_sizes=[32, 16], num_classes=2):\n",
        "        \"\"\"\n",
        "        A simple MLP classifier for phishing detection based on extracted features.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features\n",
        "            hidden_sizes (list): List of hidden layer sizes\n",
        "            num_classes (int): Number of output classes\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        # Create hidden layers\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(prev_size, num_classes))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape [batch_size, input_size]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output logits of shape [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lvV6u10clPk6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "class SmolLMClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_model_name=\"HuggingFaceTB/SmolLM2-135M\", num_classes=2):\n",
        "        \"\"\"\n",
        "        Initialize a SmolLM based classifier.\n",
        "\n",
        "        Args:\n",
        "            pretrained_model_name (str): Name of the pretrained model to load\n",
        "            num_classes (int): Number of output classes\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Load the pretrained model\n",
        "        self.transformer = AutoModel.from_pretrained(pretrained_model_name)\n",
        "\n",
        "        # Configure the classifier head\n",
        "        hidden_size = self.transformer.config.hidden_size\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Token IDs\n",
        "            attention_mask (torch.Tensor): Attention mask\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output logits of shape [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        # Get the transformer outputs\n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Use the last hidden state of the last token for classification\n",
        "        last_hidden_state = transformer_outputs.last_hidden_state\n",
        "        sequence_output = last_hidden_state[:, -1, :]  # Use the last token\n",
        "\n",
        "        # Pass through the classifier head\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YVK_dq-nlPk6"
      },
      "outputs": [],
      "source": [
        "def plot_values(epochs, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "    \"\"\"\n",
        "    Plot training progress.\n",
        "\n",
        "    Args:\n",
        "        epochs: X values for epochs\n",
        "        examples_seen: X values for examples seen\n",
        "        train_values: Y values for training\n",
        "        val_values: Y values for validation\n",
        "        label: Label to use (loss or accuracy)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot against epochs\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs.numpy(), train_values, label=f'Training {label}')\n",
        "    plt.plot(epochs.numpy(), val_values, label=f'Validation {label}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(label.capitalize())\n",
        "    plt.title(f'{label.capitalize()} vs. Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot against examples seen\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(examples_seen.numpy(), train_values, label=f'Training {label}')\n",
        "    plt.plot(examples_seen.numpy(), val_values, label=f'Validation {label}')\n",
        "    plt.xlabel('Examples Seen')\n",
        "    plt.ylabel(label.capitalize())\n",
        "    plt.title(f'{label.capitalize()} vs. Examples Seen')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'phishing_{label}_plot.png')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Ia_ppPIplPk6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoConfig\n",
        "\n",
        "class SmolLMClassifier(torch.nn.Module):\n",
        "    def __init__(self, pretrained_model_name=\"HuggingFaceTB/SmolLM2-135M\", num_classes=2):\n",
        "        super(SmolLMClassifier, self).__init__()\n",
        "        config = AutoConfig.from_pretrained(pretrained_model_name)\n",
        "        self.transformer = AutoModel.from_pretrained(pretrained_model_name, config=config)\n",
        "\n",
        "        hidden_size = config.hidden_size if hasattr(config, 'hidden_size') else config.n_embd\n",
        "        self.classifier = torch.nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Use the [CLS] token representation (first token)\n",
        "        cls_representation = outputs.last_hidden_state[:, 0, :]\n",
        "        logits = self.classifier(cls_representation)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "San-ViITlPk6"
      },
      "outputs": [],
      "source": [
        "def classify_email(features_dict, model, tokenizer, device, max_length=128):\n",
        "    \"\"\"\n",
        "    Classify email based on its features.\n",
        "\n",
        "    Args:\n",
        "        features_dict (dict): Dictionary of feature values\n",
        "        model: Trained model\n",
        "        tokenizer: Tokenizer for encoding text\n",
        "        device: Device to run on\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        int: Predicted class (0 = ham, 1 = phishing)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create text representation from features\n",
        "    text = (\n",
        "        f\"Email characteristics: \"\n",
        "        f\"Contains {features_dict.get('num_words', 0)} words with {features_dict.get('num_unique_words', 0)} unique words. \"\n",
        "        f\"Has {features_dict.get('num_stopwords', 0)} stopwords. \"\n",
        "        f\"Includes {features_dict.get('num_links', 0)} links to {features_dict.get('num_unique_domains', 0)} different domains. \"\n",
        "        f\"Contains {features_dict.get('num_email_addresses', 0)} email addresses. \"\n",
        "        f\"Has {features_dict.get('num_spelling_errors', 0)} spelling errors. \"\n",
        "        f\"Contains {features_dict.get('num_urgent_keywords', 0)} urgent keywords.\"\n",
        "    )\n",
        "\n",
        "    # Encode the text\n",
        "    encoding = tokenizer.encode(text)\n",
        "\n",
        "    # Truncate or pad to max_length\n",
        "    if len(encoding) > max_length:\n",
        "        encoding = encoding[:max_length]\n",
        "    else:\n",
        "        # Pad with EOS token ID or 0 depending on tokenizer\n",
        "        pad_token = 0  # Default\n",
        "        if hasattr(tokenizer, 'pad_token_id'):\n",
        "            pad_token = tokenizer.pad_token_id\n",
        "        encoding = encoding + [pad_token] * (max_length - len(encoding))\n",
        "\n",
        "    # Convert to tensors\n",
        "    input_ids = torch.tensor(encoding, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    attention_mask = torch.ones_like(input_ids).to(device)\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    return predicted.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sSkSxRElPk7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOfpiZ3plPk7",
        "outputId": "50496123-a4a1-4078-e9eb-e26271e57b72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Shivansh Kalra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n",
            "Loaded 9728 samples from train.csv\n",
            "Loaded 1389 samples from validation.csv\n",
            "Loaded 2781 samples from test.csv\n",
            "1216 training batches\n",
            "174 validation batches\n",
            "348 test batches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Shivansh Kalra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import tiktoken\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Determine device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load tokenizer (using the same as your instructor for consistency)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")  # Change to this\n",
        "\n",
        "    # Load the datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = PhishingDataset(csv_file=\"train.csv\", tokenizer=tokenizer)\n",
        "    val_dataset = PhishingDataset(csv_file=\"validation.csv\", tokenizer=tokenizer)\n",
        "    test_dataset = PhishingDataset(csv_file=\"test.csv\", tokenizer=tokenizer)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 8\n",
        "    num_workers = 2\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    print(f\"{len(train_loader)} training batches\")\n",
        "    print(f\"{len(val_loader)} validation batches\")\n",
        "    print(f\"{len(test_loader)} test batches\")\n",
        "\n",
        "    # Initialize the model - choose one of these:\n",
        "    # Option 1: GPT-2\n",
        "    model = SmolLMClassifier(pretrained_model_name=\"HuggingFaceTB/SmolLM2-135M\", num_classes=2)\n",
        "\n",
        "    # Option 2: SmolLM2\n",
        "    # model = SmolLMClassifier(pretrained_model_name=\"HuggingFaceTB/SmolLM2-135M\", num_classes=2)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Freeze most parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Unfreeze only the classifier and final transformer layer\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    if hasattr(model.transformer, 'h'):\n",
        "        for param in model.transformer.h[-1].parameters():\n",
        "            param.requires_grad = True\n",
        "    if hasattr(model.transformer, 'ln_f'):\n",
        "        for param in model.transformer.ln_f.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # Initial evaluation\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "        test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "\n",
        "    print(f\"Initial Training loss: {train_loss:.3f}\")\n",
        "    print(f\"Initial Validation loss: {val_loss:.3f}\")\n",
        "    print(f\"Initial Test loss: {test_loss:.3f}\")\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "    num_epochs = 1\n",
        "    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=1, eval_iter=1, max_batches_per_epoch=10\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time_minutes = (end_time - start_time) / 60\n",
        "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
        "\n",
        "    # Plot training progress\n",
        "    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "    examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "\n",
        "    plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)\n",
        "    plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")\n",
        "\n",
        "    # Final evaluation\n",
        "    train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "    val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "    test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "    print(f\"Final Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "    print(f\"Final Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "    print(f\"Final Test accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"phishing_transformer_classifier.pth\")\n",
        "    print(\"Model saved to phishing_transformer_classifier.pth\")\n",
        "\n",
        "    # Example predictions\n",
        "    phishing_features = {\n",
        "        'num_words': 150,\n",
        "        'num_unique_words': 90,\n",
        "        'num_stopwords': 40,\n",
        "        'num_links': 3,\n",
        "        'num_unique_domains': 2,\n",
        "        'num_email_addresses': 1,\n",
        "        'num_spelling_errors': 5,\n",
        "        'num_urgent_keywords': 4\n",
        "    }\n",
        "\n",
        "    prediction = classify_email(phishing_features, model, tokenizer, device, max_length=train_dataset.max_length)\n",
        "    print(f\"Phishing sample prediction (expected 1): {prediction}\")\n",
        "\n",
        "    ham_features = {\n",
        "        'num_words': 120,\n",
        "        'num_unique_words': 100,\n",
        "        'num_stopwords': 30,\n",
        "        'num_links': 0,\n",
        "        'num_unique_domains': 0,\n",
        "        'num_email_addresses': 0,\n",
        "        'num_spelling_errors': 0,\n",
        "        'num_urgent_keywords': 0\n",
        "    }\n",
        "\n",
        "    prediction = classify_email(ham_features, model, tokenizer, device, max_length=train_dataset.max_length)\n",
        "    print(f\"Ham sample prediction (expected 0): {prediction}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9rnmZPir3ng"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
