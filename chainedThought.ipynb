{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 83735,
          "databundleVersionId": 9881586,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "chainedThought",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivansh193/LLMProjs/blob/main/chainedThought.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Quf2CBZ8DuOM"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "gemini_long_context_path = kagglehub.competition_download('gemini-long-context')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "jrKPcFerDuOO"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chainedThought\n",
        "---\n",
        "Our project  introduces a sophisticated approach to complex query analysis by combining distributed multi-agent processing with an advanced chain of thought methodology. Our project represents a significant evolution in artificial intelligence query processing, leveraging both parallel agent computation and iterative refinement mechanisms.\n",
        "\n",
        "At its core, the system employs a hierarchical structure where a primary Large Language Model (Gemini) serves as both coordinator and initial processor. When a query is received, instead of generating a direct, potentially unsatisfactory response, the system initiates two parallel processes: domain decomposition and chain of thought analysis.\n",
        "\n",
        "The domain decomposition process dynamically instantiates specialized sub-agents, each focused on specific aspects of the query such as economic, geopolitical, or social dimensions. These agents operate concurrently, providing domain-specific insights that contribute to the initial response. Simultaneously, the chain of thought mechanism begins its iterative refinement cycle.\n",
        "\n",
        "This refinement process is sophisticated and methodical. The initial response undergoes a systematic criticism phase, where potential weaknesses, biases, or gaps in reasoning are identified. These criticisms then trigger a defense mechanism, where the system generates counterarguments and improvements. This dialectical process is governed by a fitness function that evaluates the quality, comprehensiveness, and logical coherence of each iteration.\n",
        "\n",
        "The system's true innovation lies in its synthesis phase. The Final Compilation module integrates the parallel insights from specialized agents with the refined outcomes of the chain of thought process. This integration ensures that the final response not only benefits from multiple domain expertise but also withstands rigorous logical scrutiny.\n",
        "\n",
        "The result is a comprehensive analysis system that combines the breadth of multi-agent processing with the depth of iterative reasoning. This dual approach ensures that complex queries receive responses that are both extensively researched across domains and thoroughly refined through logical examination.\n",
        "\n",
        "Through this architecture, our project addresses the limitations of both simple query-response systems and single-agent processors, providing a framework that can handle complex, nuanced questions with the depth and breadth they require. The system's adaptability and scalability make it particularly suitable for applications ranging from academic research to strategic business analysis.\n",
        "\n",
        "*Note- This solution truly takes advantage of Gemini's Long Context Window, as this would not have worked nearly as efficiently on a RAG alternative or other methods.*\n",
        "\n",
        "---\n",
        "\n",
        "![logic diagram](https://i.ibb.co/J37ZF7c/image.png)\n",
        "\n",
        "A Logic Diagram of the functioning of our solution."
      ],
      "metadata": {
        "id": "clB6q5HEDuOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up all imports for our submission"
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "_duVIhgbDuOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we set up all our important **imports** and **configure logging functions**, using a certain format helping us with debugging"
      ],
      "metadata": {
        "id": "tLFgZv83DuOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass\n",
        "from itertools import cycle\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "key1 = user_secrets.get_secret(\"key1\")\n",
        "key2 = user_secrets.get_secret(\"key2\")\n",
        "key3 = user_secrets.get_secret(\"key3\")\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T23:46:12.786222Z",
          "iopub.execute_input": "2024-12-01T23:46:12.78661Z",
          "iopub.status.idle": "2024-12-01T23:46:13.353144Z",
          "shell.execute_reply.started": "2024-12-01T23:46:12.786576Z",
          "shell.execute_reply": "2024-12-01T23:46:13.35187Z"
        },
        "id": "0Sdcg8FpDuOQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up classes to be used across the project."
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "5581IDRnDuOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we set up major classes to be used across the code.\n",
        "The first class APIKeyManager helps us with sharding of keys, to reduce the amount of time and credits being spent for one single domain/iterations.\n",
        "Next we use AIModel abstract class and its abstract methods to generate and identify domains\n",
        "\n",
        "Next GeminiAPI is another class which sends in the initial prompt, manages the temperature of the response, generates the response, parses the Domains, and the questions array from the initial response.\n",
        "\n",
        "Then we analyse the domain through the domain analysis class, which has all the necessary values we need for our analysis."
      ],
      "metadata": {
        "id": "DHKEDJLnDuOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class APIKeyManager:\n",
        "    \"\"\"Manages rotation of API keys to prevent rate limiting.\"\"\"\n",
        "    api_keys: List[str]\n",
        "    requests_per_key: int = 6\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if not self.api_keys:\n",
        "            raise ValueError(\"At least one API key must be provided\")\n",
        "        self.current_request_count = 0\n",
        "        self.key_cycle = cycle(self.api_keys)\n",
        "        self.current_key = next(self.key_cycle)\n",
        "        self._lock = asyncio.Lock()  # Add lock for thread safety\n",
        "\n",
        "    def get_current_key(self) -> str:\n",
        "        return self.current_key\n",
        "\n",
        "    async def next_key(self) -> str:\n",
        "        \"\"\"Rotate to next API key if request limit reached.\"\"\"\n",
        "        async with self._lock:  # Ensure thread-safe key rotation\n",
        "            self.current_request_count += 1\n",
        "\n",
        "            if self.current_request_count >= self.requests_per_key:\n",
        "                self.current_key = next(self.key_cycle)\n",
        "                self.current_request_count = 0\n",
        "                logger.info(f\"Switching to new API key: {self.current_key[:10]}...\")\n",
        "\n",
        "            return self.current_key\n",
        "\n",
        "class AIModel(ABC):\n",
        "    \"\"\"Abstract base class for AI model implementations.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    async def generate_response(self, prompt: str, temperature: float = 0.3) -> str:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    async def identify_domains(self, question: str) -> Dict[str, List[str]]:\n",
        "        pass\n",
        "\n",
        "class GeminiAPI(AIModel):\n",
        "\n",
        "    def __init__(self, api_keys: List[str]):\n",
        "        self.key_manager = APIKeyManager(api_keys)\n",
        "\n",
        "    async def generate_response(self, prompt: str, temperature: float = 0.3) -> str:\n",
        "        api_key = await self.key_manager.next_key()  # Make this async\n",
        "\n",
        "        try:\n",
        "            genai.configure(api_key=api_key)\n",
        "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "            start = time.time()\n",
        "            response = model.generate_content(prompt, generation_config={\"temperature\": temperature})\n",
        "            end = time.time()\n",
        "            logger.info(f\"Generated response in {end - start:.2f} seconds\")\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating response with key {api_key[:5]}...: {e}\")\n",
        "            raise\n",
        "\n",
        "    async def identify_domains(self, question: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Identify relevant domains and generate specific questions for analysis.\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Analyze this question and identify the relevant domains that would affect the answer:\n",
        "        \"{question}\"\n",
        "\n",
        "        Provide your response in this EXACT JSON format, with NO additional text:\n",
        "        {{\n",
        "            \"domains\": [\"domain1\", \"domain2\"],\n",
        "            \"questions\": [\"specific question 1\", \"specific question 2\"]\n",
        "        }}\n",
        "\n",
        "        Requirements:\n",
        "        1. Include 3-5 relevant domains\n",
        "        2. Each domain must be a single word\n",
        "        3. Each question must specifically relate to its domain\n",
        "        4. Questions should be analytical and specific\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = await self.generate_response(prompt, temperature=0.3)\n",
        "\n",
        "            # Clean the response - remove any non-JSON content\n",
        "            response = response.strip()\n",
        "            if not response.startswith('{'):\n",
        "                response = response[response.find('{'):]\n",
        "            if not response.endswith('}'):\n",
        "                response = response[:response.rfind('}')+1]\n",
        "\n",
        "            parsed = json.loads(response)\n",
        "\n",
        "            # Validate the response structure\n",
        "            if not isinstance(parsed, dict) or 'domains' not in parsed or 'questions' not in parsed:\n",
        "                raise ValueError(\"Invalid response structure\")\n",
        "            if len(parsed['domains']) != len(parsed['questions']):\n",
        "                raise ValueError(\"Mismatch between domains and questions count\")\n",
        "\n",
        "            return parsed\n",
        "        except (json.JSONDecodeError, ValueError) as e:\n",
        "            logger.error(f\"Error parsing domains: {e}, Response: {response}\")\n",
        "            raise  # Re-raise to handle in the coordinator\n",
        "@dataclass\n",
        "class DomainAnalysis:\n",
        "    \"\"\"Stores analysis results for a specific domain.\"\"\"\n",
        "    domain: str\n",
        "    question: str\n",
        "    current_response: Optional[str] = None\n",
        "    confidence_score: float = 0.0\n",
        "    iteration_count: int = 0"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T23:46:13.355294Z",
          "iopub.execute_input": "2024-12-01T23:46:13.355736Z",
          "iopub.status.idle": "2024-12-01T23:46:13.376467Z",
          "shell.execute_reply.started": "2024-12-01T23:46:13.35569Z",
          "shell.execute_reply": "2024-12-01T23:46:13.375111Z"
        },
        "id": "In9qvmOjDuOR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ![](http://)Setting up prompts and fitness function\n",
        "Next we set up our prompts and fitness functions.\n",
        "Domain Agent class defines the domain it is running for, the model, the max iterations, and the target confidence before the model has to stop, whichever happens first.\n",
        "\n",
        "Then we calculate the temperature of the response through a basic function\n",
        "\n",
        "Then, we analyse our iterations by arguing with itself. Through extensive defense and rebuttals like conversation enabling and making the best use of Long context.\n",
        "\n",
        "we get initial response we highlight weaknesses, oversights, etc. and use that criticism to improve the response. And using the following improve response functions to further improve upon these responses."
      ],
      "metadata": {
        "id": "zYtg7nwGDuOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DomainAgent:\n",
        "    def __init__(self, domain: str, question: str, ai_model: AIModel):\n",
        "        self.domain = domain\n",
        "        self.question = question\n",
        "        self.ai_model = ai_model\n",
        "        self.analysis = DomainAnalysis(domain=domain, question=question)\n",
        "        self.MAX_ITERATIONS = 8\n",
        "        self.TARGET_CONFIDENCE = 80\n",
        "        self._iteration_lock = asyncio.Lock()\n",
        "\n",
        "    def calculate_temperature(self, iteration: int) -> float:\n",
        "        \"\"\"Calculate temperature based on iteration number.\"\"\"\n",
        "        base_temp = 0.3\n",
        "        temp_range = 0.7\n",
        "        temp_step = temp_range / (self.MAX_ITERATIONS - 1)\n",
        "        return min(1.0, base_temp + (temp_step * iteration))\n",
        "\n",
        "    async def analyze_iteration(self, iteration: int) -> None:\n",
        "        \"\"\"Execute a single iteration of analysis.\"\"\"\n",
        "        if iteration >= self.MAX_ITERATIONS or self.analysis.confidence_score >= self.TARGET_CONFIDENCE:\n",
        "            return\n",
        "\n",
        "        async with self._iteration_lock:  # Ensure sequential iterations\n",
        "            current_temp = self.calculate_temperature(iteration)\n",
        "            logger.info(f\"Domain: {self.domain} - Iteration {iteration} - Temperature: {current_temp:.2f}\")\n",
        "\n",
        "            try:\n",
        "                # For first iteration, get initial response\n",
        "                if iteration == 0:\n",
        "                    self.analysis.current_response = await self._get_initial_response(current_temp)\n",
        "                    self.analysis.iteration_count = 1\n",
        "                    logger.info(f\"Initial response for {self.domain}: {self.analysis.current_response[:100]}...\")\n",
        "                    return\n",
        "\n",
        "                # For subsequent iterations\n",
        "                if self.analysis.current_response:\n",
        "                    criticisms = await self._get_criticisms(current_temp)\n",
        "                    defense = await self._defend_against_criticisms(criticisms, current_temp)\n",
        "                    new_response = await self._improve_response(defense, current_temp)\n",
        "                    confidence = await self._calculate_confidence(new_response, criticisms, defense)\n",
        "\n",
        "                    self.analysis.current_response = new_response\n",
        "                    self.analysis.confidence_score = confidence\n",
        "                    self.analysis.iteration_count = iteration + 1\n",
        "\n",
        "                    logger.info(f\"Domain: {self.domain} - Iteration {iteration} - \"\n",
        "                              f\"Confidence: {self.analysis.confidence_score}%\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in analysis iteration {iteration} for domain {self.domain}: {e}\")\n",
        "                raise\n",
        "\n",
        "    async def _get_initial_response(self, temperature: float) -> str:\n",
        "        prompt = f\"\"\"Analyze the following question from the perspective of {self.domain}:\n",
        "        Question: {self.question}\n",
        "        Provide a detailed and CONCISE analysis and prediction.\"\"\"\n",
        "        return await self.ai_model.generate_response(prompt, temperature)\n",
        "\n",
        "    async def _get_criticisms(self, temperature: float) -> List[str]:\n",
        "        prompt = f\"\"\"Review the CORE ARGUMENTS of this analysis critically:\n",
        "        Analysis: {self.analysis.current_response}\n",
        "        List the main weaknesses, oversights, or alternative viewpoints.  Be concise and target only the most important flaws.\n",
        "        Make sure to keep criticisms in 100-300 characters max.\"\"\"\n",
        "        criticism_response = await self.ai_model.generate_response(prompt, temperature)\n",
        "        return self._parse_criticisms(criticism_response)\n",
        "\n",
        "    def _parse_criticisms(self, criticism_response: str) -> List[str]:\n",
        "        criticisms = [c.strip() for c in criticism_response.split('\\n') if c.strip()]\n",
        "        return criticisms\n",
        "\n",
        "    async def _defend_against_criticisms(self, criticisms: List[str], temperature: float) -> Dict[str, str]:\n",
        "        defenses = {}\n",
        "        for criticism in criticisms:\n",
        "            prompt = f\"\"\"Is this a valid criticism of the CORE ARGUMENT? Reply with \"YES\" or \"NO\", followed by a concise justification.\n",
        "            Analysis: {self.analysis.current_response}\n",
        "            Criticism: {criticism}\n",
        "            Make sure to keep justifications in 100-300 characters max.\"\"\"\n",
        "            defense = await self.ai_model.generate_response(prompt, temperature)\n",
        "            defenses[criticism] = defense\n",
        "        return defenses\n",
        "\n",
        "    async def _improve_response(self, defenses: Dict[str, str], temperature: float) -> str:\n",
        "        valid_criticisms = [crit for crit, defense in defenses.items() if defense.lower().startswith(\"yes\")]\n",
        "\n",
        "        if not valid_criticisms:\n",
        "            return self.analysis.current_response\n",
        "\n",
        "        try:\n",
        "            prompt = f\"\"\"Improve the CORE ARGUMENTS of this analysis based on the VALID criticisms and your rebuttals:\n",
        "            Current analysis: {self.analysis.current_response}\n",
        "            Valid criticisms and Rebuttals: { {crit: defenses[crit] for crit in valid_criticisms} }\n",
        "            Provide only core changes to the initial analysis that addresses these criticisms concisely.\"\"\"\n",
        "\n",
        "            improved_response = await self.ai_model.generate_response(prompt, temperature)\n",
        "            return improved_response\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in _improve_response: {e}\")\n",
        "            return self.analysis.current_response\n",
        "\n",
        "    async def _calculate_confidence(self, response: str, criticisms: List[str], defenses: Dict[str, str]) -> float:\n",
        "        prompt = f\"\"\"Given the original criticisms and the rebuttals, what is the confidence level (0-100) of the improved analysis?\n",
        "        Improved Analysis: {response}\n",
        "        Criticisms and Rebuttals: {defenses}\n",
        "        Consider:\n",
        "        1. How well the CORE ARGUMENTS address the valid criticisms.\n",
        "        2. The overall strength and conciseness of the analysis.\n",
        "        Return only a number.\"\"\"\n",
        "\n",
        "        try:\n",
        "            confidence_str = await self.ai_model.generate_response(prompt)\n",
        "            return float(confidence_str.strip())\n",
        "        except (ValueError, TypeError):\n",
        "            logger.warning(\"Could not parse confidence score, defaulting to 0.0\")\n",
        "            return 0.0"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T23:46:13.378379Z",
          "iopub.execute_input": "2024-12-01T23:46:13.378689Z",
          "iopub.status.idle": "2024-12-01T23:46:13.398699Z",
          "shell.execute_reply.started": "2024-12-01T23:46:13.37866Z",
          "shell.execute_reply": "2024-12-01T23:46:13.397382Z"
        },
        "id": "Hp2hro2JDuOR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [](http://)Compiling final response\n",
        "Then we come down to compiling the final response, which is done by the class multiagent coordinator, it compiles all these responses by the main agent, after considering all the arguments and returns the response after deep analysis of all the required arguments."
      ],
      "metadata": {
        "id": "QLHb72CfDuOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiAgentCoordinator:\n",
        "    def __init__(self, ai_model: AIModel):\n",
        "        self.ai_model = ai_model\n",
        "\n",
        "    async def analyze_question(self, question: str) -> Tuple[Dict[str, DomainAnalysis], str]:\n",
        "        \"\"\"Analyze a question using multiple domain agents and synthesize results.\"\"\"\n",
        "        try:\n",
        "            # Identify domains and questions\n",
        "            domain_info = await self.ai_model.identify_domains(question)\n",
        "\n",
        "            if not domain_info or 'domains' not in domain_info:\n",
        "                raise ValueError(\"Failed to identify domains properly\")\n",
        "\n",
        "            domains = domain_info['domains']\n",
        "            questions = domain_info['questions']\n",
        "\n",
        "            logger.info(f\"Identified domains: {domains}\")\n",
        "\n",
        "            # Create agents for each domain\n",
        "            agents = {\n",
        "                domain: DomainAgent(domain, question, self.ai_model)\n",
        "                for domain, question in zip(domains, questions)\n",
        "            }\n",
        "\n",
        "            # Track results and active agents\n",
        "            results: Dict[str, DomainAnalysis] = {}\n",
        "            active_agents = agents.copy()\n",
        "\n",
        "            # Run iterations across all domains concurrently\n",
        "            for iteration in range(max(agent.MAX_ITERATIONS for agent in agents.values())):\n",
        "                logger.info(f\"Starting iteration {iteration} across all domains\")\n",
        "\n",
        "                # Run one iteration for all active agents concurrently\n",
        "                await asyncio.gather(\n",
        "                    *(agent.analyze_iteration(iteration) for agent in active_agents.values())\n",
        "                )\n",
        "\n",
        "                # Check for completed domains\n",
        "                completed_domains = set()\n",
        "                for domain, agent in active_agents.items():\n",
        "                    if (agent.analysis.confidence_score >= agent.TARGET_CONFIDENCE or\n",
        "                        iteration >= agent.MAX_ITERATIONS - 1):\n",
        "                        results[domain] = agent.analysis\n",
        "                        completed_domains.add(domain)\n",
        "\n",
        "                # Remove completed domains\n",
        "                for domain in completed_domains:\n",
        "                    active_agents.pop(domain, None)\n",
        "\n",
        "                # If all domains complete, break\n",
        "                if not active_agents:\n",
        "                    break\n",
        "\n",
        "            # Add any remaining active agents to results\n",
        "            for domain, agent in active_agents.items():\n",
        "                results[domain] = agent.analysis\n",
        "\n",
        "            if not results:\n",
        "                raise ValueError(\"No valid analyses were produced\")\n",
        "\n",
        "            # Synthesize final response\n",
        "            final_response = await self._synthesize_final_response(results)\n",
        "\n",
        "            return results, final_response\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in question analysis: {e}\")\n",
        "            raise\n",
        "\n",
        "    async def _synthesize_final_response(self, domain_analyses: Dict[str, DomainAnalysis]) -> str:\n",
        "        \"\"\"Synthesize domain-specific analyses into a final response.\"\"\"\n",
        "        analyses_text = \"\\n\\n\".join([\n",
        "            f\"Domain: {domain}\\n\"\n",
        "            f\"Confidence: {analysis.confidence_score}%\\n\"\n",
        "            f\"Analysis: {analysis.current_response}\"\n",
        "            for domain, analysis in domain_analyses.items()\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"Synthesize these domain-specific analyses into a comprehensive final response:\n",
        "        {analyses_text}\n",
        "\n",
        "        Provide a concise, well-reasoned final prediction that integrates all domain perspectives.\n",
        "        Include:\n",
        "        1. Key agreements across domains\n",
        "        2. Critical interactions between factors\n",
        "        3. Overall confidence assessment\n",
        "        4. Major uncertainties\n",
        "\n",
        "        Keep the response focused and actionable.\"\"\"\n",
        "        #         5. Important values and figures with interpretations.\n",
        "        # 6. Sources, if credible.\n",
        "\n",
        "        return await self.ai_model.generate_response(prompt)\n",
        "async def analyze_question(api_keys: List[str], question: str) -> Tuple[Dict[str, DomainAnalysis], str]:\n",
        "    \"\"\"Main entry point for question analysis.\"\"\"\n",
        "    ai_model = GeminiAPI(api_keys)\n",
        "    coordinator = MultiAgentCoordinator(ai_model)\n",
        "    return await coordinator.analyze_question(question)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T23:46:13.40128Z",
          "iopub.execute_input": "2024-12-01T23:46:13.401695Z",
          "iopub.status.idle": "2024-12-01T23:46:13.418612Z",
          "shell.execute_reply.started": "2024-12-01T23:46:13.401661Z",
          "shell.execute_reply": "2024-12-01T23:46:13.417518Z"
        },
        "id": "Bh-OraKdDuOS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An example cell using our code\n",
        "Here we try our code out with an example question, \"What are the predicted prices of coffee in 2027?\". The output gives us analyses spanning four different domains along with a comprehensive well rounded final analysis."
      ],
      "metadata": {
        "id": "Q2McJWBlDuOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example with multiple API keys\n",
        "    api_keys=[key1,key2,key3]  # Replace with actual API keys\n",
        "\n",
        "    question = \"What are the predicted prices of coffee in 2027?\"\n",
        "\n",
        "    async def main():\n",
        "        try:\n",
        "            analyses, final_response = await analyze_question(api_keys, question)\n",
        "\n",
        "            # Print results\n",
        "            for domain, analysis in analyses.items():\n",
        "                print(f\"\\n{'-'*50}\")\n",
        "                print(f\"Domain: {domain}\")\n",
        "                print(f\"Confidence: {analysis.confidence_score}%\")\n",
        "                print(f\"Analysis: {analysis.current_response}\")\n",
        "\n",
        "            print(f\"\\n{'-'*50}\\nFinal Analysis:\")\n",
        "            print(final_response)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    await main()"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T23:46:13.419853Z",
          "iopub.execute_input": "2024-12-01T23:46:13.420185Z",
          "iopub.status.idle": "2024-12-01T23:47:27.56146Z",
          "shell.execute_reply.started": "2024-12-01T23:46:13.420119Z",
          "shell.execute_reply": "2024-12-01T23:47:27.560453Z"
        },
        "id": "n1zIQTHkDuOS"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}